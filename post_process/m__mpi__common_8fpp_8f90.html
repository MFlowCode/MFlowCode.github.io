<!-- HTML header for doxygen 1.9.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SY496B9L99"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-SY496B9L99');
</script>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.16.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>MFC: Post-Process: m_mpi_common.fpp.f90 File Reference</title>
<meta name="description" content="m_mpi_common.fpp.f90 File Reference â€” MFC documentation. Open-source exascale multiphase flow solver." />
<meta name="keywords" content="exascale, fluid dynamics, cfd, computational fluid dynamics, compressible, hpc, bryngelson, colonius, subgrid, multiphase, frontier, summit, el capitan, aurora, amd gpu, gpu, nvidia"/>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js", "TeX/AMSmath.js", "TeX/AMSsymbols.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
// This file is set as MATHJAX_CODEFILE in the Doxyfile. It configures how
// MathJax renders expressions in Markdown so that it is consistent with GitHub.
MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath:  [ ['$',  '$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: "line" // Ignore code blocks: https://web.archive.org/web/20120430100225/http://www.mathjax.org/docs/1.1/options/tex2jax.html
    },
    "HTML-CSS": {
      fonts: ["TeX"]
    }
});
</script>
<script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link rel="shortcut icon" href="icon.ico" type="image/x-icon" />
<link href="doxygen-awesome.css" rel="stylesheet" type="text/css"/>
<link href="doxygen-awesome-sidebar-only.css" rel="stylesheet" type="text/css"/>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="icon.ico"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">MFC: Post-Process
   </div>
   <div id="projectbrief">Exascale flow solver</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.16.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(function(){initNavTree('m__mpi__common_8fpp_8f90.html','',''); });
</script>
<div id="container">
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="headertitle"><div class="title">m_mpi_common.fpp.f90 File Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 id="header-namespaces" class="groupheader"><a id="namespaces" name="namespaces"></a>
Modules</h2></td></tr>
<tr class="memitem:m_5Fmpi_5Fcommon" id="r_m_5Fmpi_5Fcommon"><td class="memItemLeft">module &#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html">m_mpi_common</a></td></tr>
<tr class="memdesc:namespacem__mpi__common"><td class="mdescLeft">&#160;</td><td class="mdescRight">The module serves as a proxy to the parameters and subroutines available in the MPI implementation's MPI module. Specifically, the purpose of the proxy is to harness basic MPI commands into more complicated procedures as to accomplish the communication goals for the simulation. <br /></td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 id="header-func-members" class="groupheader"><a id="func-members" name="func-members"></a>
Functions/Subroutines</h2></td></tr>
<tr class="memitem:a056c92bf55bd24a6cb43936abd313f4e" id="r_a056c92bf55bd24a6cb43936abd313f4e"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a056c92bf55bd24a6cb43936abd313f4e">m_mpi_common::s_initialize_mpi_common_module</a></td></tr>
<tr class="memdesc:a056c92bf55bd24a6cb43936abd313f4e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The computation of parameters, the allocation of memory, the association of pointers and/or the execution of any other procedures that are necessary to setup the module.  <br /></td></tr>
<tr class="memitem:a150d5a63224d7e80c1290d699fbb5855" id="r_a150d5a63224d7e80c1290d699fbb5855"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a150d5a63224d7e80c1290d699fbb5855">m_mpi_common::s_mpi_initialize</a></td></tr>
<tr class="memdesc:a150d5a63224d7e80c1290d699fbb5855"><td class="mdescLeft">&#160;</td><td class="mdescRight">The subroutine initializes the MPI execution environment and queries both the number of processors which will be available for the job and the local processor rank.  <br /></td></tr>
<tr class="memitem:a2e27b3212956af624c8da826f0c2c529" id="r_a2e27b3212956af624c8da826f0c2c529"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a2e27b3212956af624c8da826f0c2c529">m_mpi_common::s_initialize_mpi_data</a> (<a class="el" href="m__phase__change_8fpp_8f90.html#a6d2af1366163bbea81771912d405abfa">q_cons_vf</a>, ib_markers, beta)</td></tr>
<tr class="memitem:a564644bad898d940dc6caea3297bb424" id="r_a564644bad898d940dc6caea3297bb424"><td class="memItemLeft">subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a564644bad898d940dc6caea3297bb424">m_mpi_common::s_initialize_mpi_data_ds</a> (<a class="el" href="m__phase__change_8fpp_8f90.html#a6d2af1366163bbea81771912d405abfa">q_cons_vf</a>)</td></tr>
<tr class="memitem:a078e1d24059f8545abda443a92575948" id="r_a078e1d24059f8545abda443a92575948"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a078e1d24059f8545abda443a92575948">m_mpi_common::s_mpi_gather_data</a> (my_vector, counts, gathered_vector, root)</td></tr>
<tr class="memitem:a98b6d5f758850a2ba16fb7553c055827" id="r_a98b6d5f758850a2ba16fb7553c055827"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a98b6d5f758850a2ba16fb7553c055827">m_mpi_common::mpi_bcast_time_step_values</a> (proc_time, time_avg)</td></tr>
<tr class="memitem:a4d03f5fc95657b6bec1f89221295b0cc" id="r_a4d03f5fc95657b6bec1f89221295b0cc"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a4d03f5fc95657b6bec1f89221295b0cc">m_mpi_common::s_prohibit_abort</a> (condition, message)</td></tr>
<tr class="memitem:a3681f17d9e7b2cac992b6ae14aac0f15" id="r_a3681f17d9e7b2cac992b6ae14aac0f15"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a3681f17d9e7b2cac992b6ae14aac0f15">m_mpi_common::s_mpi_reduce_stability_criteria_extrema</a> (icfl_max_loc, vcfl_max_loc, rc_min_loc, icfl_max_glb, vcfl_max_glb, rc_min_glb)</td></tr>
<tr class="memdesc:a3681f17d9e7b2cac992b6ae14aac0f15"><td class="mdescLeft">&#160;</td><td class="mdescRight">The goal of this subroutine is to determine the global extrema of the stability criteria in the computational domain. This is performed by sifting through the local extrema of each stability criterion. Note that each of the local extrema is from a single process, within its assigned section of the computational domain. Finally, note that the global extrema values are only bookkeept on the rank 0 processor.  <br /></td></tr>
<tr class="memitem:a3b287a39841538011798d6f5e59d41ac" id="r_a3b287a39841538011798d6f5e59d41ac"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a3b287a39841538011798d6f5e59d41ac">m_mpi_common::s_mpi_allreduce_sum</a> (var_loc, var_glb)</td></tr>
<tr class="memdesc:a3b287a39841538011798d6f5e59d41ac"><td class="mdescLeft">&#160;</td><td class="mdescRight">The following subroutine takes the input local variable from all processors and reduces to the sum of all values. The reduced variable is recorded back onto the original local variable on each processor.  <br /></td></tr>
<tr class="memitem:a22620175f94237b04b08d371ae891467" id="r_a22620175f94237b04b08d371ae891467"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a22620175f94237b04b08d371ae891467">m_mpi_common::s_mpi_allreduce_vectors_sum</a> (var_loc, var_glb, num_vectors, vector_length)</td></tr>
<tr class="memdesc:a22620175f94237b04b08d371ae891467"><td class="mdescLeft">&#160;</td><td class="mdescRight">This subroutine follows the behavior of the s_mpi_allreduce_sum subroutine with the additional feature that it reduces an array of vectors.  <br /></td></tr>
<tr class="memitem:a750c773a9421c7837f197be9bdc4fb73" id="r_a750c773a9421c7837f197be9bdc4fb73"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a750c773a9421c7837f197be9bdc4fb73">m_mpi_common::s_mpi_allreduce_integer_sum</a> (var_loc, var_glb)</td></tr>
<tr class="memdesc:a750c773a9421c7837f197be9bdc4fb73"><td class="mdescLeft">&#160;</td><td class="mdescRight">The following subroutine takes the input local variable from all processors and reduces to the sum of all values. The reduced variable is recorded back onto the original local variable on each processor.  <br /></td></tr>
<tr class="memitem:a40f3c1242840a184d017acef92cce7d4" id="r_a40f3c1242840a184d017acef92cce7d4"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a40f3c1242840a184d017acef92cce7d4">m_mpi_common::s_mpi_allreduce_min</a> (var_loc, var_glb)</td></tr>
<tr class="memdesc:a40f3c1242840a184d017acef92cce7d4"><td class="mdescLeft">&#160;</td><td class="mdescRight">The following subroutine takes the input local variable from all processors and reduces to the minimum of all values. The reduced variable is recorded back onto the original local variable on each processor.  <br /></td></tr>
<tr class="memitem:a65728020613f88febe83c74836abb371" id="r_a65728020613f88febe83c74836abb371"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a65728020613f88febe83c74836abb371">m_mpi_common::s_mpi_allreduce_max</a> (var_loc, var_glb)</td></tr>
<tr class="memdesc:a65728020613f88febe83c74836abb371"><td class="mdescLeft">&#160;</td><td class="mdescRight">The following subroutine takes the input local variable from all processors and reduces to the maximum of all values. The reduced variable is recorded back onto the original local variable on each processor.  <br /></td></tr>
<tr class="memitem:a9e8820cd853294229c309298cb44ad2b" id="r_a9e8820cd853294229c309298cb44ad2b"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a9e8820cd853294229c309298cb44ad2b">m_mpi_common::s_mpi_reduce_min</a> (var_loc)</td></tr>
<tr class="memdesc:a9e8820cd853294229c309298cb44ad2b"><td class="mdescLeft">&#160;</td><td class="mdescRight">The following subroutine takes the inputted variable and determines its minimum value on the entire computational domain. The result is stored back into inputted variable.  <br /></td></tr>
<tr class="memitem:a2e3bdaffd98ab167782c6f43f182f89e" id="r_a2e3bdaffd98ab167782c6f43f182f89e"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a2e3bdaffd98ab167782c6f43f182f89e">m_mpi_common::s_mpi_reduce_maxloc</a> (var_loc)</td></tr>
<tr class="memdesc:a2e3bdaffd98ab167782c6f43f182f89e"><td class="mdescLeft">&#160;</td><td class="mdescRight">The following subroutine takes the first element of the 2-element inputted variable and determines its maximum value on the entire computational domain. The result is stored back into the first element of the variable while the rank of the processor that is in charge of the sub- domain containing the maximum is stored into the second element of the variable.  <br /></td></tr>
<tr class="memitem:a04cfc0bdb377a0ce2f9fd478e4bb3807" id="r_a04cfc0bdb377a0ce2f9fd478e4bb3807"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a04cfc0bdb377a0ce2f9fd478e4bb3807">m_mpi_common::s_mpi_abort</a> (prnt, code)</td></tr>
<tr class="memdesc:a04cfc0bdb377a0ce2f9fd478e4bb3807"><td class="mdescLeft">&#160;</td><td class="mdescRight">The subroutine terminates the MPI execution environment.  <br /></td></tr>
<tr class="memitem:a1073fc1fcb6f3b9394606d5c9285e9f0" id="r_a1073fc1fcb6f3b9394606d5c9285e9f0"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a1073fc1fcb6f3b9394606d5c9285e9f0">m_mpi_common::s_mpi_barrier</a></td></tr>
<tr class="memdesc:a1073fc1fcb6f3b9394606d5c9285e9f0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Halts all processes until all have reached barrier.  <br /></td></tr>
<tr class="memitem:a50d271a607262a2f55d7a00f7e692e38" id="r_a50d271a607262a2f55d7a00f7e692e38"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a50d271a607262a2f55d7a00f7e692e38">m_mpi_common::s_mpi_finalize</a></td></tr>
<tr class="memdesc:a50d271a607262a2f55d7a00f7e692e38"><td class="mdescLeft">&#160;</td><td class="mdescRight">The subroutine finalizes the MPI execution environment.  <br /></td></tr>
<tr class="memitem:a489959387c97e03d8e09eba0ed6280ec" id="r_a489959387c97e03d8e09eba0ed6280ec"><td class="memItemLeft">subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a489959387c97e03d8e09eba0ed6280ec">m_mpi_common::s_mpi_sendrecv_variables_buffers</a> (q_comm, mpi_dir, pbc_loc, nvar, pb_in, mv_in)</td></tr>
<tr class="memdesc:a489959387c97e03d8e09eba0ed6280ec"><td class="mdescLeft">&#160;</td><td class="mdescRight">The goal of this procedure is to populate the buffers of the cell-average conservative variables by communicating with the neighboring processors.  <br /></td></tr>
<tr class="memitem:afe54fe25c12eda7afe195fbf0fa166fc" id="r_afe54fe25c12eda7afe195fbf0fa166fc"><td class="memItemLeft">subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#afe54fe25c12eda7afe195fbf0fa166fc">m_mpi_common::s_mpi_decompose_computational_domain</a></td></tr>
<tr class="memdesc:afe54fe25c12eda7afe195fbf0fa166fc"><td class="mdescLeft">&#160;</td><td class="mdescRight">The purpose of this procedure is to optimally decompose the computational domain among the available processors. This is performed by attempting to award each processor, in each of the coordinate directions, approximately the same number of cells, and then recomputing the affected global parameters.  <br /></td></tr>
<tr class="memitem:a30bc077674966874fec000b8afcbf2e9" id="r_a30bc077674966874fec000b8afcbf2e9"><td class="memItemLeft">subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a30bc077674966874fec000b8afcbf2e9">m_mpi_common::s_mpi_sendrecv_grid_variables_buffers</a> (mpi_dir, pbc_loc)</td></tr>
<tr class="memdesc:a30bc077674966874fec000b8afcbf2e9"><td class="mdescLeft">&#160;</td><td class="mdescRight">The goal of this procedure is to populate the buffers of the grid variables by communicating with the neighboring processors. Note that only the buffers of the cell-width distributions are handled in such a way. This is because the buffers of cell-boundary locations may be calculated directly from those of the cell-width distributions.  <br /></td></tr>
<tr class="memitem:ab9e557649b1fca986ee793073abae4d4" id="r_ab9e557649b1fca986ee793073abae4d4"><td class="memItemLeft">impure subroutine&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#ab9e557649b1fca986ee793073abae4d4">m_mpi_common::s_finalize_mpi_common_module</a></td></tr>
<tr class="memdesc:ab9e557649b1fca986ee793073abae4d4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Module deallocation and/or disassociation procedures.  <br /></td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 id="header-var-members" class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a8f9044f9e60879f65ae11de6de95c213" id="r_a8f9044f9e60879f65ae11de6de95c213"><td class="memItemLeft">integer, private&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a8f9044f9e60879f65ae11de6de95c213">m_mpi_common::v_size</a></td></tr>
<tr class="memitem:a3deaeac1e239299664faa426e15f3285" id="r_a3deaeac1e239299664faa426e15f3285"><td class="memItemLeft">real(<a class="el" href="namespacem__precision__select.html#a5c61bc246bcd3d3239269c38473c000a">wp</a>), dimension(:), allocatable, private&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a3deaeac1e239299664faa426e15f3285">m_mpi_common::buff_send</a></td></tr>
<tr class="memdesc:a3deaeac1e239299664faa426e15f3285"><td class="mdescLeft">&#160;</td><td class="mdescRight">This variable is utilized to pack and send the buffer of the cell-average primitive variables, for a single computational domain boundary at the time, to the relevant neighboring processor.  <br /></td></tr>
<tr class="memitem:a2427f77cebecdbb0d52fafc4b458b369" id="r_a2427f77cebecdbb0d52fafc4b458b369"><td class="memItemLeft">real(<a class="el" href="namespacem__precision__select.html#a5c61bc246bcd3d3239269c38473c000a">wp</a>), dimension(:), allocatable, private&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#a2427f77cebecdbb0d52fafc4b458b369">m_mpi_common::buff_recv</a></td></tr>
<tr class="memdesc:a2427f77cebecdbb0d52fafc4b458b369"><td class="mdescLeft">&#160;</td><td class="mdescRight">buff_recv is utilized to receive and unpack the buffer of the cell- average primitive variables, for a single computational domain boundary at the time, from the relevant neighboring processor.  <br /></td></tr>
<tr class="memitem:af2cb4d00741a037730c041b4c5795b2e" id="r_af2cb4d00741a037730c041b4c5795b2e"><td class="memItemLeft">integer(kind=8)&#160;</td><td class="memItemRight"><a class="el" href="namespacem__mpi__common.html#af2cb4d00741a037730c041b4c5795b2e">m_mpi_common::halo_size</a></td></tr>
</table>
</div><!-- contents -->
</div><!-- doc-content -->
<div id="page-nav" class="page-nav-panel">
<div id="page-nav-resize-handle"></div>
<div id="page-nav-tree">
<div id="page-nav-contents">
</div><!-- page-nav-contents -->
</div><!-- page-nav-tree -->
</div><!-- page-nav -->
</div><!-- container -->
<!-- HTML footer for doxygen 1.9.1-->
<!-- start footer part -->
</body>
</html>
